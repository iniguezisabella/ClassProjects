---
title: "What factors influence the best times to rent a bike?"
date: ''
output:
  html_document: 
    theme: readable
    toc: yes
  pdf_document: default
urlcolor: cyan
---

# Package Loading

Below are all the packages used in the analysis/output of this project.

```{r}
library(readr)
library(stringr)
library(faraway)
library(MASS)
```

# Introduction

One of the most popular forms of transportation in recent years is the use of a bikeshare utility, such as those provided by Lyft, Veo, etc. This is mainly for those who live in highly urbanized areas where:

-   Most important locations such as work, school, or shopping are accessible by foot, bike, or public transportation, and the usefulness of a car is negated.
-   Individuals are conscious of their carbon footprint and would rather utilize forms of transportation that do not require burning hydrocarbons.
-   It is not economically feasible or wise to own a vehicle.
-   Commuters are more health-aware and want/need a form of transport that would allow for a form of daily exercise.

For the end user, a major inconvenince is the ability to locate a bike and to determine when these are available. This allows them to plan commute times effectively, as being aware of how easily they can obtain a bike at any given time would allow them to create ample buffer time for their travels. They may have to consider time of day, day of the week, weather, etc. in order to best ascertain whether or not a bike is easily accessible and creating a utility to help predict this would reduce the amount of stress associated with the dependence on a bikeshare utility.

For businesses, especially the bikeshare utilities themselves, having a method of determining the rate of bike usage could be useful for them to:

-   Determine the best times to launch advertising campaigns.
-   Determine the amount of bikes to stock and whether these stocks require more bikes/replacements.
-   Determine peak hours and when they should expect times for repairs.
-   Determine times of days that commuters will be traveling so that other businesses can promote their own products (drinks, food, etc.).

In this study, we seek to create a model that will be able to predict rates of bike rentals in bikes per hour so that all parties involved can make educated decisions regarding this bikeshare information. The method used with be multiple linear regression using the sum of least squares method. In particular, we would like to create a model which:

-   Follows the form of a multiple linear regression, with the form $Y = \beta_0 + \beta_1x_{i1} + \beta_2x_{i2} + \cdots + \beta_{p - 1}x_{i(p - 1)} + \epsilon_i$, where $i = 1, 2, \cdots, n$, $n = \text{number of predictors}$, and $\epsilon_i = N(0, \sigma ^ 2)$. This means that the model will have $p - 1$ predictor variables and any error observed (deviation from the line generated by the model, called residuals) are normally distributed with a standard deviation of $\sigma ^ 2$ and are independent and identically distributed.
-   This model is statistically significant to make predictions that behave similarly to data.
-   This model obeys all assumptions of linear regression, mainly:
    -   The response variable, $Y$, can be written as a linear combination of the predictors with noise about this relationship.
    -   The errors are independent of each other and of the $\beta$ parameters.
    -   The distribution of the errors truly follow a normal distribution, i.e., $\epsilon_0 = N(0, \sigma ^ 2)$.
    -   The variance is the same at any set of predictor values.

The dataset we seek to use comes from the [UC Irvine Machine Learning Repository](https://archive.ics.uci.edu), and is titled [Seoul Bike Sharing Demand](https://archive.ics.uci.edu/dataset/560/seoul+bike+sharing+demand) which contains 8760 observations. This dataset, donated on 2/29/2020, contains the count of public bicycles rented per hour in the Seoul Bike Sharing System in Seoul, South Korea, and corresponding weather data and holiday information, mainly:

-   `Date`, the year-month-day of rental.
-   `Rented Bike Count`, the count of bikes rented per hour (the response variable, $Y$).
-   `Hour`, the hour of the day.
-   `Temperature(°C)`, the temperature in Celcius.
-   `Humidity(%)`, the humidity of the atmosphere in %.
-   `Wind speed (m/s)`, the speed of the wind in m/s.
-   `Visibility (10m)`, the visibility of the hour recorded in units of 10 meters.
-   `Dew point temperature (°C)`, the temperature air needs to be at to achieve a relative humidity of 100%, recorded in Celsius.
-   `Solar radiation (MJ/m2)`, the energy radiated by the sun, given in $MJ/m ^ 2$ (megajoules per square meter).
-   `Rainfall (mm)`, the amount of rainfall in mm.
-   `Snowfall (cm)`, the amount of snowfall in cm.
-   `Seasons`, the season of the day.
-   `Holiday`, whether or not the day is a holiday (recorded as `No Holiday` or `Holiday`).
-   `Functioning Day`, whether or not the day was a work day (recorded as `Yes` or `No`).

The methodologies used to determine and assess models is based on the function of the model: whether it be to explain the relationship between predictors and the response or to predict new data based off of old data, the latter being less restrictive and supports larger models.

Since the use of this model is mostly for predictive purposes, the methodologies reflected will be focused on generating predictions, namely using model selection heuristics that are used for prediction (AIC over BIC model selection, see the method section), generating prediction intervals for the model as well to create a range of bike rental rates for a given day, and determining the significance of the regression and possible confounding factors (collinearity, violations to the assumptions of linear regression, etc.).

# Methods

## Data Cleaning

The data utilized can be directly downloaded from [this link](https://archive.ics.uci.edu/static/public/560/seoul+bike+sharing+demand.zip). The data will be imported into `R` using the `read_csv()` function from the `readr` package. But as will soon be seen, there is a problem with importing the raw data (this data is included in the directory of this project).

```{r, eval = FALSE}
# Attempting to load the raw data into R. 
bike_data = read_csv("SeoulBikeData.csv")
```

The main problem with this data is that it contains non UTF-8 characters which `read_csv()` cannot recognize this, and running this code directly will result in an error (to knit this file using `R` Markdown, the running of this code was suppressed). In order to fix this, the file needs to conform to UTF-8 format, which can be done in many ways (there is likely a way to do it in `R`), however the method utilized is converting it using Microsoft Excel. The output of this conversion is also in this project's directory, but if you would like to do this yourself, all that must be done is saving the file as .csv UTF-8 in the "Save As" menu of Excel. This file is saved as `SeoulBikeDataUTF8.csv`. Because the column names have spaces making them difficult to work with, we will also be replacing them:

-   `Date` -\> `date`\
-   `Rented Bike Count` -\> `rentedbikecount`
-   `Hour` -\> `hour`
-   `Temperature(°C)` -\> `temperature`
-   `Humidity(%)` -\> `humidity`
-   `Wind speed (m/s)` -\> `windspeed`
-   `Visibility (10m)` -\> `visibility`
-   `Dew point temperature (°C)` -\> `dptemp`
-   `Solar radiation (MJ/m2)` -\> `solarrad`
-   `Rainfall (mm)` -\> `rainfall`
-   `Snowfall (cm)` -\> `snowfall`
-   `Seasons` -\> `seasons`
-   `Holiday` -\> `holiday`
-   `Functioning Day` -\> `functioning`

```{r}
# Loding the data into R.
bike_data = read_csv("SeoulBikeDataUTF8.csv", show_col_types = FALSE)

# Renaming the columns in the dataset to R-friendly names.  
names(bike_data) = c("date", "rentedbikecount", "hour", "temperature", "humidity", "windspeed", "visibility", "dptemp", "solarrad", "rainfall", "snowfall", "seasons", "holiday", "functioning")

# Displaying the first 10 observations of the dataset.
head(bike_data, n = 10)

# Showing the titles of variables in the dataset.
names(bike_data)
```

Now we have successfully imported the data. Let's ensure that every observation has values for every value and exclude those that do not (these will have NA values in place of this).

```{r}
# Determining the sample size of the dataset. 
nrow(bike_data)

# Checking for NA observations.
all(!is.na(bike_data))
```

There were no NA values detected so values for every variable were recorded for every observation. Just to reiterate, the response variable will be `Rented Bike Count` and we will create a model that best generates predictions for this. Because the `Date` variable is not necessarily numerical and a establishing a relationship between `Rented Bike Count` and `Date` would require further philosophical discussion to properly utilize it in the model, it will be excluded.

```{r}
# Subsetting all columns except the Date column.
bike_data = bike_data[, !names(bike_data) %in% c("date")]

# Checking the variables in the bike_data dataset.
names(bike_data)

# Displaying the first 10 observations in dataset.
head(bike_data, n = 10)
```

## Excluding Outliers

We would like to exclude observations with a high amount of influence on the regression, namely those with high leverage and high residuals. We can do this by fitting a test regrsesion including all variables and calculating Cook's distance, where high influence is a Cook's distance greater than 4 ($D_i > 4$).

First we fit a temp model (which is used purely for excluding the outliers and not considered in generating a final model) and then using this temp model exclude points that affect this regression greatly:

```{r}
# Generating the temp model. 
temp_model = lm(rentedbikecount ~ ., data = bike_data)

# Calculating cook's distances for each point and excluding high influence points. 
for (i in 1:nrow(bike_data)) {
  if (cooks.distance(temp_model)[i] > 4 / length(cooks.distance(temp_model))) {
    bike_data = bike_data[-i, ]
  }
}

# Showing new size of dataset.
nrow(bike_data)
```

The dataset has been significantly reduced (`r 8760 - 8407` observations removed).

## Splitting the Data

Next we need to split the data in half, half to generate the model (training) and the other to evaluate how effective the model is (testing). Because the number of rows is odd, we would like to split so that `bike_data_trn` has more samples (so that the model regression has more data to work with).

```{r}
# Setting a seed for consistency of results.
set.seed(42)

# Creating a vector of random indeces used to index.
bike_data_trn_idx = sample(1:nrow(bike_data), ceiling(nrow(bike_data) / 2))

# Subsetting the data into two subsets.
bike_data_trn = bike_data[bike_data_trn_idx, ]
bike_data_tst = bike_data[-bike_data_trn_idx, ]

# Verifying successful split by adding the rows and comparing to the original bike_data dataset. 
all(nrow(bike_data_trn) + nrow(bike_data_tst) == nrow(bike_data))
```

For the training of the model, the `bike_data_trn` dataset will be used for most of the methods section and testing the model in the results section will be done with the `bike_data_tst` dataset.

## The Full Additive Model

### Exploring Collinearity

First we would like to build our model by testing a fully additive model, which includes all possible predictors. We would like to assess the extent of collinearity in our dataset, but there is too much data for a `pairs()` plot to be useful, and subsetting could accidentally leave out important information (unless we ran simulations thousands of times). Obtaining correlation coefficients for all the numeric variables would be ideal for determining collinearity.

```{r}
# Obtaining correlations between predictors.
round(cor(bike_data_trn[, 1:10]), 2)
```

Most of the predictors seem to be independent from each other, except for the correlation between `temperature` and `dptemp` (the dew point temperature). While explaining the relationship between the two is far beyond the scope of this analysis, it makes intuitive sense that two variables that are related to the temperature correlate with one another. Another possible collinearity issue is between `humidity` and `visibility`, `dptemp`, and `solarrad`, so we will explore those as well.

Let's verify this by finding variance inflation factors for all the variables using the `vif()` function. This value is essentially demonstrates the effect that variables have on each other - the larger, the more likely collinearity is an issue. We will use the heuristic of $\text{VIF} > 5$ to determine whether collinearity is an issue for a given variable.

We'll first construct the full additive model which includes all predictors and then look at the VIF's.

```{r}
# Creating the full additive model. 
full_additive_model = lm(rentedbikecount ~ ., data = bike_data_trn)

# Obtaining vif's for all variables.
vif(full_additive_model)
```

As we can see, the spurious variables are `temperature`, `humidity`, and `dptemp`. We can verify this by obtaining a partial correlation coefficient and confirm via a variable added plot. These are likely collinear with each other, but it would be a mistake to exclude all of these from our analysis. From these three, intuitively speaking, `temperature` is the best variable to leave in and test the remaining two variables.

A partial correlation coefficient essentially compares two models - the regression of the model against all predictors except the variables in question, and the variable in question against the other predictors. We will do this one at a time and exclude variables with small correlation coefficients.

-   `dptemp`

```{r}
# Generating the a full additive model except dptemp. 
additive_nodptemp_small = lm(rentedbikecount ~ . - dptemp, data = bike_data_trn)

# Generating a model with only the questionable variables. 
additive_dptemp_small = lm(dptemp ~ . - rentedbikecount, data = bike_data_trn)

# Obtaining the partial correlation coefficient.
dptemp_pcc = cor(resid(additive_nodptemp_small), resid(additive_dptemp_small))

# Displaying the partial correlation coefficient
dptemp_pcc
```

-   `humidity`

```{r}
# Generating the a full additive model except humidity. 
additive_nohumid_small = lm(rentedbikecount ~ . - humidity, data = bike_data_trn)

# Generating a model with only the questionable variables. 
additive_humid_small = lm(humidity ~ . - rentedbikecount, data = bike_data_trn)

# Obtaining the partial correlation coefficient.
humidity_pcc = cor(resid(additive_nohumid_small), resid(additive_humid_small))

# Displaying the partial correlation coefficient
humidity_pcc
```

Both partial correlation coefficients are small, so we should consider excluding these from our model. Let's verify this using variable added plots: \* `dptemp`

```{r}
# Creating a variable added plot for dptemp
plot(resid(additive_nodptemp_small) ~ resid(additive_dptemp_small), 
     col = "dodgerblue", pch = 20,
     xlab = "Residuals, Added Predictor",
     ylab = "Residuals, Excluding Predictor",
     main = "Variable Added Plot (dptemp/dew point temperature)")
abline(h = 0, lty = 2)
abline(v = 0, lty = 2)
abline(lm(resid(additive_dptemp_small) ~ resid(additive_nodptemp_small)),
       col = "darkorange", lwd = 2)
```

-   `humidity`

```{r}
# Creating a variable added plot for dptemp
plot(resid(additive_nohumid_small) ~ resid(additive_humid_small), 
     col = "dodgerblue", pch = 20,
     xlab = "Residuals, Added Predictor",
     ylab = "Residuals, Excluding Predictor",
     main = "Variable Added Plot (humid/humidity)")
abline(h = 0, lty = 2)
abline(v = 0, lty = 2)
abline(lm(resid(additive_humid_small) ~ resid(additive_nohumid_small)),
       col = "darkorange", lwd = 2)
```

Here we can see that variable added plots for both variables have no linear relationship, so adding `humidity` or `dptemp` to our model should not be considered. As a reuslt, our current additive model includes the linear combination of all possible predictors except `humidity` or `dptemp`.

And so the model as a result of this section is:

```{r}
# Generating an additive model excluding humidity and dptemp.
collinear_clean_model = lm(rentedbikecount ~ . - humidity - dptemp, data = bike_data_trn)

# Displaying the summary of this model.
summary(collinear_clean_model)
```

## Exploring Interaction Terms

We would like to now see if there is any interaction between the predictors that may affect our data. Let's explore two-way interactions between numerical predictors - going further than that would require more thought about how the terms might interact (via intelligently selecting interaction terms with real-life knowledge of how these terms might interact), interactions between dummy variables would lead to quite complex analysis, and interaction between 3 or more variables would require extensive computational resources which will not run on a machine in a timely manner. The resulting model would also be extremely large.

Let's first generate this model (still excluding the terms from the collinearity section of this analysis):

```{r}
# Generating the model. 
int_model = lm(rentedbikecount ~ (. - humidity - dptemp - seasons - holiday - functioning) ^ 2 + seasons + holiday + functioning, data = bike_data_trn)

# Displaying the summary of the model.
summary(int_model)
```

As we can see this model is quite large, with coefficients having varying levels of significance, so we would like to minimize the unnecessary coefficients as much as possible. Many of the $\beta$ parameters have NA values, which indicates that there is high collinearity among these new $\beta$ parameters. As these are categorical, calculating VIF's would be quite difficult as it is outside the scope of this analysis. Instead, we will be skipping straight to model selection via backwards BIC.

We use backwards BIC because:

-   Backwards: It is better to consider the full model, especially when the number of estimated $\beta$ parameters is not substantial enough to make this computationally expensive. Since our goal is to make a predictive model, it is better to consider all $\beta$ parameters simultaneously.
-   BIC: Because of the higher parsimony penalty, this will minimize as many variables as it can in order to create a selected model. This should help with some of the collinearity that might be observed in these new $\beta$ parameters.

Let's run backwards BIC on this dataset:

```{r}
# Generating the backwards_bic_model. 
backwards_bic_model = step(int_model, direction = "backward", n = log(nrow(bike_data_trn)), trace = 0)

# Displaying 
summary(backwards_bic_model)
```

Many of the parameters with low significance were excluded, leaving us with a model with mostly high significance estimates. It would be unwise to exclude the variables on single $t$-tests, as many of those excluded (such as $hour$) are definitely important.

## Transformations

To determine whether transformations should be considered, let's run a fitted versus residuals plot alongside a normal Q-Q plot followed by a Shapiro-Wilk Test to see if constant variance is not violated.

-   Q-Q plot

```{r}
qqnorm(resid(backwards_bic_model), main = "Normal Q-Q Plot, fit_1", col = "darkgrey")
qqline(resid(backwards_bic_model), col = "dodgerblue", lwd = 2)
```

-   Fitted vs. Residuals Plot

```{r}
plot(fitted(backwards_bic_model), resid(backwards_bic_model), col = "grey", pch = 20,
     xlab = "Fitted", ylab = "Residuals", main = "Data from BIC Model")
abline(h = 0, col = "darkorange", lwd = 2)
```

-   Shapiro-Wilk Test

```{r}
shapiro.test(resid(backwards_bic_model))$p.value
```

Transformations of those introduced in STAT420 did not help improve these results (increasing shapiro test p-values, or helping the plots normalize) so transformations will not be applied to the dataset.

## Models Available

There are four models for us to choose from, namely

-   `full_additive_model` - the additive model including all predictors.
-   `collinear_clean_model` - the additive model that excludes likely collinear predictors.
-   `int_model` - the full additive model and the two-way interactions.
-   `backwards_bic_model` - the interaction model that was selected using a parsimonious selection method.

In the results section, we seek to select a model that best predicts the dataset. We will do this by selecting a model that has the highest statistical significance (because of the way these models were constructed, they are all nested under the `int_model`), best passes linear regression assumption tests (Shapiro-Wilk, Q-Q plot, fitted vs. residuals plot). We will then explore the way these models can be used and the implications in the discussion section.

# Results

## Testing Anova F-test

```{r}
anova(full_additive_model, collinear_clean_model)$"Pr(>F)"
```

Based on the $p-value$ of 1.17e-52 we fail to reject the null. We prefer the collinear model which is just the additive model without the collinear predictors.

```{r}
anova(collinear_clean_model, int_model)$"Pr(>F)"
```

Based on the $p-value$ of 2.51e-131 we fail to reject the null. We prefer the interactive model over the cleaned additive model.

```{r}
anova(int_model, backwards_bic_model)$"Pr(>F)"
```

Based on the $p-value$ of 0.992 we reject the null. Which means we prefer the interactive model over the BIC model.

## Testing Shapiro Wilk

```{r}
shap_test = function(model, alpha=0.05){
  # p-value and decision
  p_val = shapiro.test(resid(model))$p.value
  decision = ifelse(p_val < alpha, "Reject", "Fail to Reject")
  list(p_val = p_val, decision = decision)
}
```

```{r}
shap_test(full_additive_model)
shap_test(collinear_clean_model)
shap_test(int_model)
shap_test(backwards_bic_model)
```

## Testing Q-Q Plots and Fitted Residuals

```{r}
plotting = function(model, pcol = "grey", lcol = "dodgerblue", alpha = 0.05, 
                       plotit = TRUE) {
  
  if (plotit == TRUE) {
    
    # side-by-side plots (one row, two columns)
    par(mfrow = c(1, 2))
    
    # fitted versus residuals
    plot(fitted(model), resid(model), 
         col = pcol, pch = 20, cex = 1.5, 
         xlab = "Fitted", ylab = "Residuals", 
         main = "Fitted versus Residuals")
    abline(h = 0, col = lcol, lwd = 2)
    grid()
    
    # qq-plot
    qqnorm(resid(model), col = pcol, pch = 20, cex = 1.5)
    qqline(resid(model), col = lcol, lwd = 2)
    grid()
  }
}
```

```{r fig.height=5, fig.width=10}
plotting(full_additive_model)
```

```{r fig.height=5, fig.width=10}
plotting(collinear_clean_model)
```

```{r fig.height=5, fig.width=10}
plotting(int_model)
```

```{r fig.height=5, fig.width=10}
plotting(backwards_bic_model)
```

# Discussion

Having concluded via Anova F-tests and chosen to use the interactive model, we are now able to predict rates of bike rentals based on the most influential factors so that those involved in the bikeshare business can make educated and effective decisions in a multitude of areas.

In fitting our model, we choose to exclude observations with a high amount of influence on the regression (e.g high leverage and high residuals). We choose to do this as a small number of data points with large influence can result in violations of regression assumptions, which would then make any insights yielded by this model be inefficient or misleading. Such outliers may have come from days with unusual weather, special traffic issues, or social events that were not marked as a holiday, resulting in an unusual dip or surge in the rates of bike rentals.

We also addressed potential conflicts with collinearity by choosing to leave out the `humidity` and `dptemp` parameters from our model. Collinearity hurts the model’s ability to interpret coefficients effectively as they would have influences from other parameters, thus reducing it’s power to identify significant independent variables. This would especially impact possible interaction parameters and cause the model to wildly overfit. Not addressing it would possibly result in overfitting to the point that we make predictions far away from the mean response, further reducing the accuracy of the model.

We included interaction variables in our final model as they enabled us to examine whether the relationship between bike rentals and weather events changes depending on the value of another weather variable. This makes sense when considering ideal conditions for cycling, a combination of weather events (and not each one exclusively) may affect the rate of bike rentals more than one alone.

We also choose to use backward BIC. Backwards search because it allowed for the consideration of the full model and $\beta$ parameters simultaneously. Then BIC because of the higher parsimony penalty, this will minimize as many variables as it can in order to create a selected model. Given that the dataset is relatively large, a parsimonious selection method is better, since we would like to prevent overfitting to keep our prediction margins relatively small (helps address previously mentioned collinearity concerns).

The more we experimented with this data, the more we realized that the relationship here was likely not linear and cannot be properly represented by linear regression. It seems that stabilization of the dataset using transformations actually worsened the linear regression. So there is still more to do to further improve this model; however, it is outside the scope of this class.

# Appendix

This analysis was created by:

-   **Omar Abdelqader** ([omarza2\@illinois.edu](mailto:omarza2@illinois.edu){.email})
-   **Emil Cacayan** ([cacayan2\@illinois.edu](mailto:cacayan2@illinois.edu){.email})
-   **Isabella Iniguez** ([iinig2\@illinois.edu](mailto:iinig2@illinois.edu){.email})

The data utilized was the [Seoul Bike Sharing Demand](https://archive.ics.uci.edu/dataset/560/seoul+bike+sharing+demand) curated by the [UC Irvine Machine Learning Repository](https://archive.ics.uci.edu).
